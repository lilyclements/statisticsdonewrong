<?xml version="1.0" encoding="UTF-8"?>
<chapter xml:id="ch-significant-differences" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>When Differences in Significance Aren't Significant Differences</title>

  <p>
    <q>We compared treatments A and B with a placebo. Treatment A showed a
    significant benefit over placebo, while treatment B had no statistically
    significant benefit. Therefore, treatment A is better than treatment
    B.</q>
  </p>

  <p>
    We hear this all the time. It's an easy way of comparing medications,
    surgical interventions, therapies, and experimental results. It's
    straightforward. It seems to make sense.
  </p>

  <p>
    However, a difference in significance does not always make a significant
    difference.<xref ref="gelman-2006bj"/>
  </p>

  <p>
    One reason is the arbitrary nature of the <m>p \lt 0.05</m> cutoff. We
    could get two very similar results, with <m>p = 0.04</m> and
    <m>p = 0.06</m>, and mistakenly say they're clearly different from each
    other simply because they fall on opposite sides of the cutoff. The
    second reason is that <em>p</em> values are not measures of effect size,
    so similar <em>p</em> values do not always mean similar effects. Two
    results with identical statistical significance can nonetheless
    contradict each other.
  </p>

  <p>
    Instead, think about statistical power. If we compare our new
    experimental drugs Fixitol and Solvix to a placebo but we don't have
    enough test subjects to give us good statistical power, then we may fail
    to notice their benefits. If they have identical effects but we have
    only 50% power, then there's a good chance we'll say Fixitol has
    significant benefits and Solvix does not. Run the trial again, and it's
    just as likely that Solvix will appear beneficial and Fixitol will not.
  </p>

  <p>
    Instead of independently comparing each drug to the placebo, we should
    compare them against each other. We can test the hypothesis that they
    are equally effective, or we can construct a confidence interval for the
    extra benefit of Fixitol over Solvix. If the interval includes zero,
    then they could be equally effective; if it doesn't, then one medication
    is a clear winner. This doesn't improve our statistical power, but it
    does prevent the false conclusion that the drugs are different. Our
    tendency to look for a difference in significance should be replaced by
    a check for the significance of the difference.
  </p>

  <p>
    Examples of this error in common literature and news stories abound. A
    huge proportion of papers in neuroscience, for instance, commit the
    error.<xref ref="nieuwenhuis-2011dm"/> You might also remember a study a few years ago suggesting that
    men with more biological older brothers are more likely to be
    homosexual.<xref ref="bogaert-2006tc"/> How did they reach this conclusion? And why older brothers
    and not older sisters?
  </p>

  <p>
    The authors explain their conclusion by noting that they ran an analysis
    of various factors and their effect on homosexuality. Only the number of
    older brothers had a statistically significant effect; number of older
    sisters, or number of nonbiological older brothers, had no statistically
    significant effect.
  </p>

  <p>
    But as we've seen, that doesn't guarantee that there's a significant
    difference between the effects of older brothers and older sisters. In
    fact, taking a closer look at the data, it appears there's no
    statistically significant difference between the effect of older
    brothers and older sisters. Unfortunately, not enough data was published
    in the paper to allow a direct calculation.<xref ref="gelman-2006bj"/>
  </p>

  <section xml:id="sec-confidence-intervals">
    <title>When Significant Differences Are Missed</title>

    <p>
      The problem can run the other way. Scientists routinely judge whether a
      significant difference exists simply by eye, making use of plots like
      this one:
    </p>

    <figure xml:id="fig-confidence">
      <image source="../_images/confidence.png">
        <description>Two group means plotted with overlapping confidence intervals.</description>
      </image>
    </figure>

    <p>
      Imagine the two plotted points indicate the estimated time until
      recovery from some disease in two different groups of patients, each
      containing ten patients. There are three different things those error
      bars could represent:
    </p>

    <ol>
      <li>
        <p>
          The standard deviation of the measurements. Calculate how far each
          observation is from the average, square each difference, and then
          average the results and take the square root. This is the standard
          deviation, and it measures how spread out the measurements are from
          their mean.
        </p>
      </li>
      <li>
        <p>
          The standard error of some estimator. For example, perhaps the
          error bars are the standard error of the mean. If I were to measure
          many different samples of patients, each containing exactly
          <em>n</em> subjects, I can estimate that 68% of the mean times to
          recover I measure will be within one standard error of <q>real</q>
          average time to recover. (In the case of estimating means, the
          standard error is the standard deviation of the measurements
          divided by the square root of the number of measurements, so the
          estimate gets better as you get more data <mdash/> but not too
          fast.) Many statistical techniques, like least-squares regression,
          provide standard error estimates for their results.
        </p>
      </li>
      <li>
        <p>
          The confidence interval of some estimator. A 95% confidence
          interval is mathematically constructed to include the true value
          for 95 random samples out of 100, so it spans roughly two standard
          errors in each direction. (In more complicated statistical models
          this may not be exactly true.)
        </p>
      </li>
    </ol>

    <p>
      These three options are all different. The standard deviation is a
      simple measurement of my data. The standard error tells me how a
      statistic, like a mean or the slope of a best-fit line, would likely
      vary if I take many samples of patients. A confidence interval is
      similar, with an additional guarantee that 95% of 95% confidence
      intervals should include the <q>true</q> value.
    </p>

    <p>
      In the example plot, we have two 95% confidence intervals which
      overlap. Many scientists would view this and conclude there is no
      statistically significant difference between the groups. After all,
      groups 1 and 2 <em>might not</em> be different <mdash/> the average
      time to recover could be 25 in both groups, for example, and the
      differences only appeared because group 1 was lucky this time. But
      does this mean the difference is not statistically significant? What
      would the <em>p</em> value be?
    </p>

    <p>
      In this case, <m>p \lt 0.05</m>. There is a statistically significant
      difference between the groups, even though the confidence intervals
      overlap.<fn>This was calculated with an unpaired <em>t</em> test,
      based on a standard error of 2.5 in group 1 and 3.5 in group 2.</fn>
    </p>

    <p>
      Unfortunately, many scientists skip hypothesis tests and simply glance
      at plots to see if confidence intervals overlap. This is actually a
      much more conservative test <mdash/> requiring confidence intervals to
      not overlap is akin to requiring <m>p \lt 0.01</m> in some cases.<xref ref="schenker-2001cr"/> It
      is easy to claim two measurements are not significantly different even
      when they are.
    </p>

    <p>
      Conversely, comparing measurements with standard errors or standard
      deviations will also be misleading, as standard error bars are shorter
      than confidence interval bars. Two observations might have standard
      errors which do not overlap, and yet the difference between the two is
      not statistically significant.
    </p>

    <p>
      A survey of psychologists, neuroscientists and medical researchers
      found that the majority made this simple error, with many scientists
      confusing standard errors, standard deviations, and confidence
      intervals.<xref ref="belia-2005dg"/> Another survey of climate science papers found that a
      majority of papers which compared two groups with error bars made the
      error.<xref ref="lanzante-2005hi"/> Even introductory textbooks for experimental scientists, such
      as <em>An Introduction to Error Analysis</em>, teach students to judge
      by eye, hardly mentioning formal hypothesis tests at all.
    </p>

    <p>
      There are, of course, formal statistical procedures which generate
      confidence intervals which <em>can</em> be compared by eye, and even
      correct for multiple comparisons automatically. For example, Gabriel
      comparison intervals are easily interpreted by eye.<xref ref="gabriel-1978fp"/>
    </p>

    <p>
      Overlapping confidence intervals do not mean two values are not
      significantly different. Similarly, separated standard error bars do
      not mean two values <em>are</em> significantly different. It's always
      best to use the appropriate hypothesis test instead. Your eyeball is
      not a well-defined statistical procedure.
    </p>

  </section>

</chapter>
