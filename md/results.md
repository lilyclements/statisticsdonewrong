<span id="wrought"></span>

# What have we wrought?<a href="#what-have-we-wrought" class="headerlink"
title="Permalink to this headline">¶</a>

I’ve painted a grim picture. But anyone can pick out small details in
published studies and produce a tremendous list of errors. Do these
problems matter?

Well, yes. I wouldn’t have written this otherwise.

John Ioannidis’s famous article “Why Most Published Research Findings
are
False”<span class="citation"><sup><a href="zbibliography.html#citation-ioannidis-2005bw"
class="reference internal">31</a></sup></span> was grounded in
mathematical concerns rather than an empirical test of research results.
If most research articles have poor statistical power – and
<a href="power.html#power" class="reference internal"><span
class="std std-ref">they do</span></a> – while researchers have the
freedom to choose among multitudes of analyses methods to get favorable
results – and
<a href="freedom.html#freedom" class="reference internal"><span
class="std std-ref">they do</span></a> – when most tested hypotheses are
false and most true hypotheses correspond to very small effects, we are
mathematically determined to get a multitude of false positives.

But if you want empiricism, you can have it, courtesy of John Ioannidis
and Jonathan Schoenfeld. They studied the question “Is everything we eat
associated with
cancer?”<span class="citation"><sup><a href="zbibliography.html#citation-schoenfeld-2013fq"
class="reference internal">51</a></sup></span><a href="#ontology" id="id1" class="footnote-reference">[1]</a>
After choosing fifty common ingredients out of a cookbook, they set out
to find studies linking them to cancer rates – and found 216 studies on
forty different ingredients. Of course, most of the studies disagreed
with each other. Most ingredients had multiple studies claiming they
increased *and* decreased the risk of getting cancer. Most of the
statistical evidence was weak, and meta-analyses usually showed much
smaller effects on cancer rates than the original studies.

Of course, being contradicted by follow-up studies and meta-analyses
doesn’t prevent a paper from being cited as though it were true. Even
effects which have been contradicted by massive follow-up trials with
unequivocal results are frequently cited five or ten years later, with
scientists apparently not noticing that the results are
false.<span class="citation"><sup><a href="zbibliography.html#citation-tatsioni-2007cr"
class="reference internal">55</a></sup></span> Of course, new findings
get widely publicized in the press, while contradictions and corrections
are hardly ever
mentioned.<span class="citation"><sup><a href="zbibliography.html#citation-gonon-2012do"
class="reference internal">23</a></sup></span> You can hardly blame the
scientists for not keeping up.

Let’s not forget the merely biased results. Poor reporting standards in
medical journals mean studies testing new treatments for schizophrenia
can neglect to include the scale they used to evaluate symptoms – a
handy source of bias, as trials using unpublished scales tend to produce
better results than those using previously validated
tests.<span class="citation"><sup><a href="zbibliography.html#citation-marshall-2000hg"
class="reference internal">40</a></sup></span> Other medical studies
simply
<a href="hiding.html#omit-details" class="reference internal"><span
class="std std-ref">omit particular results</span></a> if they’re not
favorable or interesting, biasing subsequent meta-analyses to only
include positive results. A third of meta-analyses are estimated to
suffer from this
problem.<span class="citation"><sup><a href="zbibliography.html#citation-kirkham-2010kj"
class="reference internal">34</a></sup></span>

Another review compared meta-analyses to subsequent large randomized
controlled trials, considered the gold standard in medicine. In over a
third of cases, the randomized trial’s outcome did not correspond well
to the
meta-analysis.<span class="citation"><sup><a href="zbibliography.html#citation-lelorier-1997ww"
class="reference internal">39</a></sup></span> Other comparisons of
meta-analyses to subsequent research found that most results were
inflated, with perhaps a fifth representing false
positives.<span class="citation"><sup><a href="zbibliography.html#citation-pereira-2011eg"
class="reference internal">45</a></sup></span>

Let’s not forget the multitude of physical science papers which misuse
confidence
intervals.<span class="citation"><sup><a href="zbibliography.html#citation-lanzante-2005hi"
class="reference internal">37</a></sup></span> Or the peer-reviewed
psychology paper allegedly providing evidence for psychic powers, on the
basis of uncontrolled multiple comparisons in exploratory
studies.<span class="citation"><sup><a href="zbibliography.html#citation-wagenmakers-2011tp"
class="reference internal">58</a></sup></span> Unsurprisingly, results
failed to be replicated – by scientists who appear not to have
calculated the statistical power of their
tests.<span class="citation"><sup><a href="zbibliography.html#citation-galak-2012fd"
class="reference internal">20</a></sup></span>

We have a problem. Let’s work on fixing it.

<table id="ontology" class="docutils footnote" data-frame="void"
data-rules="none">
<tbody data-valign="top">
<tr>
<td class="label"><a href="#id1" class="fn-backref">[1]</a></td>
<td>An important part of the ongoing <a
href="http://dailymailoncology.tumblr.com/"
class="reference external">Oncological Ontology</a> project to
categorize everything into two categories: that which cures cancer and
that which causes it.</td>
</tr>
</tbody>
</table>
