# Introduction<a href="#introduction" class="headerlink"
title="Permalink to this headline">¶</a>

In the final chapter of his famous book *How to Lie with Statistics*,
Darrell Huff tells us that “anything smacking of the medical profession”
or published by scientific laboratories and universities is worthy of
our trust – not unconditional trust, but certainly more trust than we’d
afford the media or shifty politicians. After all, Huff filled an entire
book with the misleading statistical trickery used in politics and the
media, but few people complain about statistics done by trained
professional scientists. Scientists seek understanding, not ammunition
to use against political opponents.

Statistical data analysis is fundamental to science. Open a random page
in your favorite medical journal and you’ll be deluged with statistics:
*t* tests, *p* values, proportional hazards models, risk ratios,
logistic regressions, least-squares fits, and confidence intervals.
Statisticians have provided scientists with tools of enormous power to
find order and meaning in the most complex of datasets, and scientists
have embraced them with glee.

They have not, however, embraced statistics *education*, and many
undergraduate programs in the sciences require no statistical training
whatsoever.

Since the 1980s, researchers have described numerous statistical
fallacies and misconceptions in the popular peer-reviewed scientific
literature, and have found that many scientific papers – perhaps more
than half – fall prey to these errors. Inadequate statistical power
renders many studies incapable of finding what they’re looking for;
multiple comparisons and misinterpreted *p* values cause numerous false
positives; flexible data analysis makes it easy to find a correlation
where none exists. The problem isn’t fraud but poor statistical
education – poor enough that some scientists conclude that most
published research findings are probably
false.<span class="citation"><sup><a href="zbibliography.html#citation-ioannidis-2005bw"
class="reference internal">31</a></sup></span>

What follows is a list of the more egregious statistical fallacies
regularly committed in the name of science. It assumes no knowledge of
statistical methods, since many scientists receive no formal statistical
training. And be warned: once you learn the fallacies, you will see them
*everywhere.* Don’t be alarmed. This isn’t an excuse to reject all
modern science and return to bloodletting and leeches – it’s a call to
improve the science we rely on.

<span id="changelog"></span>

## Changes<a href="#changes" class="headerlink"
title="Permalink to this headline">¶</a>

Updated January 2013 with a relevant example of the base-rate fallacy:
<a href="p-value.html#base-rate-gun" class="reference internal"><span
class="std std-ref">survey estimates of gun usage</span></a>.

Updated April 2013 with more details on
<a href="regression.html#truth-inflation"
class="reference internal"><span class="std std-ref">the interaction of
truth inflation and early stopping rules</span></a>,
<a href="p-value.html#red-herrings" class="reference internal"><span
class="std std-ref">researcher freedom in neuroscience</span></a>,
<a href="power.html#power-underpowered" class="reference internal"><span
class="std std-ref">poor statistical power in neuroscience</span></a>,
<a href="p-value.html#false-discovery" class="reference internal"><span
class="std std-ref">how to control the false discovery rate</span></a>,
<a href="hiding.html#hiding-data" class="reference internal"><span
class="std std-ref">publication bias and poor reporting</span></a>,
<a href="power.html#rtor" class="reference internal"><span
class="std std-ref">underpowered studies and right turn on
red</span></a>,
<a href="significant-differences.html#confidence-intervals"
class="reference internal"><span class="std std-ref">the misuses of
confidence intervals</span></a>,
<a href="results.html#wrought" class="reference internal"><span
class="std std-ref">the impact of all these errors</span></a>,
<a href="what-next.html#what-next" class="reference internal"><span
class="std std-ref">what can be done to save statistics</span></a>, and
additional references and details in many other places.

<span id="id1"></span>

## Contact<a href="#contact" class="headerlink"
title="Permalink to this headline">¶</a>

I’ve tried my best, but inevitably this guide will contain errors and
omissions. If you spot an error, have a question, or know a common
fallacy I’ve missed, email me at alex at refsmmat dot com.

## Acknowledgements<a href="#acknowledgements" class="headerlink"
title="Permalink to this headline">¶</a>

Thanks to Dr. James Scott, whose statistics course gave me the
background necessary to write this; to Matthew Watson and CharonY, who
gave invaluable feedback and suggestions as I wrote my drafts; to my
parents, who gave suggestions and feedback; to Dr. Brent Iverson, whose
seminar first motivated me to learn about statistical abuse; and to all
the scientists and statisticians who have broken the rules and given me
a reason to write.

Any errors in explanations are my own.

## Copyright note<a href="#copyright-note" class="headerlink"
title="Permalink to this headline">¶</a>

This work is licensed under a
<a href="http://creativecommons.org/licenses/by/3.0/"
class="reference external">Creative Commons Attribution 3.0 Unported
License</a>. You’re free to print it, copy it, translate it, rewrite it,
set it to music, slice it, dice it, or whatever, so long as you
attribute the original to me, Alex Reinhart, and provide a link back to
this site. (If you do translate it, please let me know! I’d happily
provide a link to your translation.) Hit the link to the license for
more details.

The xkcd cartoon used inside is available under the
<a href="http://creativecommons.org/licenses/by-nc/2.5/"
class="reference external">Creative Commons Attribution-NonCommercial
2.5 License</a>, and may not be used commercially without permission
from the author.
<a href="http://xkcd.com/license.html" class="reference external">More
details.</a>
