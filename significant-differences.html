
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  
<!-- Mirrored from www.statisticsdonewrong.com/significant-differences.html by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 26 Feb 2026 14:54:52 GMT -->
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>When differences in significance aren’t significant differences &#8212; Statistics Done Wrong</title>
    <link rel="stylesheet" href="_static/book.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0th',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link href="http://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic" rel="stylesheet" type="text/css">
    <link rel="canonical" href="significant-differences.html" />
    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Stopping rules and regression to the mean" href="regression.html" />
    <link rel="prev" title="The p value and the base rate fallacy" href="p-value.html" />
 
  </head>
  <body>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="when-differences-in-significance-aren-t-significant-differences">
<h1>When differences in significance aren’t significant differences<a class="headerlink" href="#when-differences-in-significance-aren-t-significant-differences" title="Permalink to this headline">¶</a></h1>
<p>“We compared treatments A and B with a placebo. Treatment A showed a significant
benefit over placebo, while treatment B had no statistically significant
benefit. Therefore, treatment A is better than treatment B.”</p>
<p>We hear this all the time. It’s an easy way of comparing medications, surgical
interventions, therapies, and experimental results. It’s straightforward. It
seems to make sense.</p>
<p>However, a difference in significance does not always make a significant
difference.<span class="citation"><sup><a class="reference internal" href="zbibliography.html#citation-gelman-2006bj">22</a></sup></span></p>
<p>One reason is the arbitrary nature of the <span class="math">\(p &lt; 0.05\)</span> cutoff. We could get
two very similar results, with <span class="math">\(p = 0.04\)</span> and <span class="math">\(p = 0.06\)</span>, and
mistakenly say they’re clearly different from each other simply because they
fall on opposite sides of the cutoff. The second reason is that <em>p</em> values are
not measures of effect size, so similar <em>p</em> values do not always mean similar
effects. Two results with identical statistical significance can nonetheless
contradict each other.</p>
<p>Instead, think about statistical power. If we compare our new experimental drugs
Fixitol and Solvix to a placebo but we don’t have enough test subjects to give
us good statistical power, then we may fail to notice their benefits. If they
have identical effects but we have only 50% power, then there’s a good chance
we’ll say Fixitol has significant benefits and Solvix does not. Run the trial
again, and it’s just as likely that Solvix will appear beneficial and Fixitol
will not.</p>
<p>Instead of independently comparing each drug to the placebo, we should compare
them against each other. We can test the hypothesis that they are equally
effective, or we can construct a confidence interval for the extra benefit of
Fixitol over Solvix. If the interval includes zero, then they could be equally
effective; if it doesn’t, then one medication is a clear winner. This doesn’t
improve our statistical power, but it does prevent the false conclusion that the
drugs are different. Our tendency to look for a difference in significance
should be replaced by a check for the significance of the difference.</p>
<p>Examples of this error in common literature and news stories abound. A huge
proportion of papers in neuroscience, for instance, commit the
error.<span class="citation"><sup><a class="reference internal" href="zbibliography.html#citation-nieuwenhuis-2011dm">44</a></sup></span> You might also remember a study a few years
ago suggesting that men with more biological older brothers are more likely to
be homosexual.<span class="citation"><sup><a class="reference internal" href="zbibliography.html#citation-bogaert-2006tc">9</a></sup></span> How did they reach this conclusion? And
why older brothers and not older sisters?</p>
<p>The authors explain their conclusion by noting that they ran an analysis of
various factors and their effect on homosexuality. Only the number of older
brothers had a statistically significant effect; number of older sisters, or
number of nonbiological older brothers, had no statistically significant effect.</p>
<p>But as we’ve seen, that doesn’t guarantee that there’s a significant difference
between the effects of older brothers and older sisters. In fact, taking a
closer look at the data, it appears there’s no statistically significant
difference between the effect of older brothers and older sisters.
Unfortunately, not enough data was published in the paper to allow a direct
calculation.<span class="citation"><sup><a class="reference internal" href="zbibliography.html#citation-gelman-2006bj">22</a></sup></span></p>
<div class="section" id="when-significant-differences-are-missed">
<span id="confidence-intervals"></span><span id="index-0"></span><h2>When significant differences are missed<a class="headerlink" href="#when-significant-differences-are-missed" title="Permalink to this headline">¶</a></h2>
<p>The problem can run the other way. Scientists routinely judge whether a
significant difference exists simply by eye, making use of plots like this one:</p>
<div class="figure">
<img alt="Two group means plotted with overlapping confidence intervals" src="_images/confidence.png" />
</div>
<p>Imagine the two plotted points indicate the estimated time until recovery from
some disease in two different groups of patients, each containing ten
patients. There are three different things those error bars could represent:</p>
<ol class="arabic simple">
<li>The standard deviation of the measurements. Calculate how far each
observation is from the average, square each difference, and then average the
results and take the square root. This is the standard deviation, and it
measures how spread out the measurements are from their mean.</li>
<li>The standard error of some estimator. For example, perhaps the error bars are
the standard error of the mean. If I were to measure many different samples
of patients, each containing exactly <em>n</em> subjects, I can estimate that 68% of
the mean times to recover I measure will be within one standard error of
“real” average time to recover. (In the case of estimating means, the
standard error is the standard deviation of the measurements divided by the
square root of the number of measurements, so the estimate gets better as you
get more data – but not too fast.) Many statistical techniques, like
least-squares regression, provide standard error estimates for their results.</li>
<li>The confidence interval of some estimator. A 95% confidence interval is
mathematically constructed to include the true value for 95 random samples
out of 100, so it spans roughly two standard errors in each direction. (In
more complicated statistical models this may not be exactly true.)</li>
</ol>
<p>These three options are all different. The standard deviation is a simple
measurement of my data. The standard error tells me how a statistic, like a mean
or the slope of a best-fit line, would likely vary if I take many samples of
patients. A confidence interval is similar, with an additional guarantee that
95% of 95% confidence intervals should include the “true” value.</p>
<p>In the example plot, we have two 95% confidence intervals which overlap. Many
scientists would view this and conclude there is no statistically significant
difference between the groups. After all, groups 1 and 2 <em>might not</em> be
different – the average time to recover could be 25 in both groups, for
example, and the differences only appeared because group 1 was lucky this
time. But does this mean the difference is not statistically significant?  What
would the <a class="reference internal" href="data-analysis.html#p-values"><span class="std std-ref">p value</span></a> be?</p>
<p>In this case, <span class="math">\(p&lt; 0.05\)</span>. There is a statistically significant difference
between the groups, even though the confidence intervals overlap.<a class="footnote-reference" href="#ttest" id="id1">[1]</a></p>
<p>Unfortunately, many scientists skip hypothesis tests and simply glance at plots
to see if confidence intervals overlap. This is actually a much more
conservative test – requiring confidence intervals to not overlap is akin to
requiring <span class="math">\(p &lt; 0.01\)</span> in some cases.<span class="citation"><sup><a class="reference internal" href="zbibliography.html#citation-schenker-2001cr">50</a></sup></span> It is easy
to claim two measurements are not significantly different even when they are.</p>
<p>Conversely, comparing measurements with standard errors or standard deviations
will also be misleading, as standard error bars are shorter than confidence
interval bars. Two observations might have standard errors which do not overlap,
and yet the difference between the two is not statistically significant.</p>
<p>A survey of psychologists, neuroscientists and medical researchers found that
the majority made this simple error, with many scientists confusing standard
errors, standard deviations, and confidence intervals.<span class="citation"><sup><a class="reference internal" href="zbibliography.html#citation-belia-2005dg">6</a></sup></span>
Another survey of climate science papers found that a majority of papers which
compared two groups with error bars made the error.<span class="citation"><sup><a class="reference internal" href="zbibliography.html#citation-lanzante-2005hi">37</a></sup></span>
Even introductory textbooks for experimental scientists, such as <em>An
Introduction to Error Analysis</em>, teach students to judge by eye, hardly
mentioning formal hypothesis tests at all.</p>
<p>There are, of course, formal statistical procedures which generate confidence
intervals which <em>can</em> be compared by eye, and even correct for <a class="reference internal" href="p-value.html#multiple-comparisons"><span class="std std-ref">multiple
comparisons</span></a> automatically. For example, Gabriel
comparison intervals are easily interpreted by eye.<span class="citation"><sup><a class="reference internal" href="zbibliography.html#citation-gabriel-1978fp">19</a></sup></span></p>
<p>Overlapping confidence intervals do not mean two values are not significantly
different. Similarly, separated standard error bars do not mean two values <em>are</em>
significantly different. It’s always best to use the appropriate hypothesis test
instead. Your eyeball is not a well-defined statistical procedure.</p>
<table class="docutils footnote" frame="void" id="ttest" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td>This was calculated with an unpaired <em>t</em> test, based on a standard
error of 2.5 in group 1 and 3.5 in group 2.</td></tr>
</tbody>
</table>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<p class="logo"><a href="index-2.html">
  <img class="logo" width="220" height="330" 
       src="_static/cover.png" alt="Logo"/>
</a></p>
<div id="book_ad">
  <h3>There's a book!</h3>
    <p class="topless">
      The revised and expanded <cite>Statistics Done Wrong</cite>, with three times as
      many statistical errors and examples, is
      <a href="http://www.nostarch.com/statsdonewrong">available in print and
        eBook!</a> An essential book for any scientist, data scientist, or statistician.
    </p>
    <p><a class="button" href="http://www.nostarch.com/statsdonewrong">Buy it!</a></p>
    <p>(or use <a href="http://www.amazon.com/Statistics-Done-Wrong-Woefully-Complete/dp/1593276206/">Amazon</a>,
      <a href="http://www.indiebound.org/book/9781593276201">IndieBound</a>, <a href="http://www.bookdepository.com/Statistics-Done-Wrong-Alex-Reinhart/9781593276201">Book Depository</a>,
      or <a href="http://www.barnesandnoble.com/w/statistics-done-wrong-alex-reinhart/1120359162?ean=9781593276201">BN</a>.)</p>
    <p>(or <a href="https://mitp.de/IT-WEB/Statistik/Statistics-Done-Wrong-Deutsche-Ausgabe.html">Deutsch</a>,
    <a href="http://www.aladin.co.kr/shop/wproduct.aspx?ItemId=64707803">한국어</a>,
    <a href="https://www.amazon.it/dp/885180284X/">Italiano</a>,
    <a href="https://www.amazon.cn/dp/B01LY7GFOD/">中文 (简体)</a>,
    <a href="http://www.cite.com.tw/book?id=75600">中文 (繁體)</a>,
    <a href="http://www.keisoshobo.co.jp/book/b272873.html">日本語</a>.)
    </p>
</div>

<h3><a href="index-2.html">Table Of Contents</a></h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="data-analysis.html">An introduction to data analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="power.html">Statistical power and underpowered statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="pseudoreplication.html">Pseudoreplication: choose your data wisely</a></li>
<li class="toctree-l1"><a class="reference internal" href="p-value.html">The <em>p</em> value and the base rate fallacy</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">When differences in significance aren’t significant differences</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#when-significant-differences-are-missed">When significant differences are missed</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="regression.html">Stopping rules and regression to the mean</a></li>
<li class="toctree-l1"><a class="reference internal" href="freedom.html">Researcher freedom: good vibrations?</a></li>
<li class="toctree-l1"><a class="reference internal" href="mistakes.html">Everybody makes mistakes</a></li>
<li class="toctree-l1"><a class="reference internal" href="hiding.html">Hiding the data</a></li>
<li class="toctree-l1"><a class="reference internal" href="results.html">What have we wrought?</a></li>
<li class="toctree-l1"><a class="reference internal" href="what-next.html">What can be done?</a></li>
<li class="toctree-l1"><a class="reference internal" href="conclusion.html">Conclusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="zbibliography.html">Bibliography</a></li>
</ul>

<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="https://www.statisticsdonewrong.com/search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
<div class="related">
  <h3>Navigation</h3>
  <ul>
    
    <li><a href="p-value.html">Previous</a></li>
    
    
    <li class="right">Next chapter: <a href="regression.html">Stopping rules and regression to the mean</a></li>

  </ul>
</div>

  <div class="footer">
    <a rel="license"
    href="https://creativecommons.org/licenses/by/4.0/"><img
    alt="Creative Commons License" style="border-width:0"
    src="../licensebuttons.net/l/by/4.0/88x31.png" /></a><br /><span
    style="font-style:italic" xmlns:dct="http://purl.org/dc/terms/" property="dct:title">Statistics Done Wrong</span> by <a xmlns:cc="http://creativecommons.org/ns#"
    property="cc:attributionName" href="https://www.refsmmat.com/">Alex Reinhart</a> is licensed under a <a
    rel="license"
    href="https://creativecommons.org/licenses/by/4.0/">Creative
    Commons Attribution 4.0 International License</a>.
  </div>
  
    <div class="footer" role="contentinfo">
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.6.7.
    </div>

  </body>

<!-- Mirrored from www.statisticsdonewrong.com/significant-differences.html by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 26 Feb 2026 14:54:52 GMT -->
</html>